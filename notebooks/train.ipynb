{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/k7/anaconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [04:26<00:00, 66.61s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "MODEL_PATH = \"google/gemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2362.89 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5849.01 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 33254.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "TRAIN_DF_FILE = \"../external/nbroad-v2.csv\"\n",
    "df = pd.read_csv(TRAIN_DF_FILE)\n",
    "df_1 = df[:1000]\n",
    "\n",
    "data = Dataset.from_pandas(df_1)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"original_text\"]), batched=True)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"rewritten_text\"]), batched=True)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"]), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formatting function for training\n",
    "def formatting_func(example):\n",
    "    text = f\"Original Essay:\\n{example['original_text'][0]}\\n\\nRewritten Essay:\\n{example['rewritten_text'][0]}\\n\\nInstruction:\\n Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model. You are trying to understand how the original essay was transformed into a new version.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay. Only give me the PROMPT. Start directly with the prompt, that's all I need. Output should be only line ONLY.\\n\\nResponse: \\n{example['rewrite_prompt'][0]}\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "TEST_DF_FILE = \"/path/to/llm-prompt-recovery/test.csv\"\n",
    "SUB_DF_FILE = \"/path/to/llm-prompt-recovery/sample_submission.csv\"\n",
    "\n",
    "tdf = pd.read_csv(TEST_DF_FILE, usecols=[\"id\", \"original_text\", \"rewritten_text\"])\n",
    "sub = pd.read_csv(SUB_DF_FILE, usecols=[\"id\", \"rewrite_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts for the test data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pbar = tqdm(total=tdf.shape[0])\n",
    "\n",
    "it = iter(tdf.iterrows())\n",
    "idx, row = next(it, (None, None))\n",
    "\n",
    "DEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n",
    "\n",
    "res = []\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "while idx is not None:\n",
    "    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatting_func({\"original_text\": [row[\"original_text\"]], \"rewritten_text\": [row[\"rewritten_text\"]], \"rewrite_prompt\": [\"\"]})\n",
    "        }]\n",
    "        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_output = peft_model.generate(encoded_input, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n",
    "        decoded_output = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", \"\", decoded_output, 1)\n",
    "\n",
    "        res.append([row[\"id\"], decoded_output])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "\n",
    "    finally:\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated prompts\n",
    "sub = pd.DataFrame(res, columns=[\"id\", \"rewrite_prompt\"])\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
