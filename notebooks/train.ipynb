{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/Users/kaiqu/anaconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kaiqu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
=======
      "/tmp/k7/anaconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig\n",
    "import platform\n",
    "import pandas as pd\n",
    "\n",
    "EXT_DATA_ROOT = \"../external/\" \n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    use_mps_device=False if platform.system() == \"Linux\" else True\n",
    ")"
=======
    "# Import necessary modules\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import datetime"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.mistral.configuration_mistral.MistralConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Add LoRA to the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply_lora(lora_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[0;32m--> 564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.mistral.configuration_mistral.MistralConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [04:26<00:00, 66.61s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:45<00:00, 11.38s/it]\n"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
<<<<<<< HEAD
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Add LoRA to the model\n",
    "model = model.apply_lora(lora_config)"
=======
    "MODEL_PATH = \"google/gemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2362.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5849.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 33254.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "TRAIN_DF_FILE = \"../external/nbroad-v2.csv\"\n",
    "df = pd.read_csv(TRAIN_DF_FILE)\n",
    "df_1 = df[:1000]\n",
    "\n",
    "data = Dataset.from_pandas(df_1)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"original_text\"]), batched=True)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"rewritten_text\"]), batched=True)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"]), batched=True)\n"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Prepare the training data\n",
    "nbroad_dataset = pd.read_csv(f\"{EXT_DATA_ROOT}/nbroad_mistral.csv\")\n",
    "original_sentences = nbroad_dataset['original_text'].tolist()  # List of original sentences\n",
    "generated_sentences = nbroad_dataset['rewritten_text'].tolist()  # List of corresponding generated sentences\n",
    "prompts = nbroad_dataset['rewrite_prompt'].tolist() # List of corresponding prompts"
=======
    "# Define the formatting function for training\n",
    "def formatting_func(example):\n",
    "    text = f\"Original Essay:\\n{example['original_text'][0]}\\n\\nRewritten Essay:\\n{example['rewritten_text'][0]}\\n\\nInstruction:\\n Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model. You are trying to understand how the original essay was transformed into a new version.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay. Only give me the PROMPT. Start directly with the prompt, that's all I need. Output should be only line ONLY.\\n\\nResponse: \\n{example['rewrite_prompt'][0]}\"\n",
    "    return [text]"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "# Tokenize the training data\n",
    "original_encodings = tokenizer(original_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "generated_encodings = tokenizer(generated_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "prompt_encodings = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Define the data collator\n",
    "class DataCollatorForPromptRecovery:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        original_batch = self.tokenizer.pad([{\"input_ids\": e[\"original_input_ids\"], \"attention_mask\": e[\"original_attention_mask\"]} for e in examples], return_tensors=\"pt\")\n",
    "        generated_batch = self.tokenizer.pad([{\"input_ids\": e[\"generated_input_ids\"], \"attention_mask\": e[\"generated_attention_mask\"]} for e in examples], return_tensors=\"pt\")\n",
    "        prompt_batch = self.tokenizer.pad([{\"input_ids\": e[\"prompt_input_ids\"], \"attention_mask\": e[\"prompt_attention_mask\"]} for e in examples], return_tensors=\"pt\")\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": original_batch[\"input_ids\"],\n",
    "            \"attention_mask\": original_batch[\"attention_mask\"],\n",
    "            \"labels\": prompt_batch[\"input_ids\"],\n",
    "            \"decoder_input_ids\": generated_batch[\"input_ids\"],\n",
    "            \"decoder_attention_mask\": generated_batch[\"attention_mask\"],\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorForPromptRecovery(tokenizer)"
=======
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset={\"original\": original_encodings, \"generated\": generated_encodings, \"prompt\": prompt_encodings},\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model with LoRA\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"lora_prompt_recovery\")"
   ]
=======
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "TEST_DF_FILE = \"/path/to/llm-prompt-recovery/test.csv\"\n",
    "SUB_DF_FILE = \"/path/to/llm-prompt-recovery/sample_submission.csv\"\n",
    "\n",
    "tdf = pd.read_csv(TEST_DF_FILE, usecols=[\"id\", \"original_text\", \"rewritten_text\"])\n",
    "sub = pd.read_csv(SUB_DF_FILE, usecols=[\"id\", \"rewrite_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts for the test data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pbar = tqdm(total=tdf.shape[0])\n",
    "\n",
    "it = iter(tdf.iterrows())\n",
    "idx, row = next(it, (None, None))\n",
    "\n",
    "DEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n",
    "\n",
    "res = []\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "while idx is not None:\n",
    "    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatting_func({\"original_text\": [row[\"original_text\"]], \"rewritten_text\": [row[\"rewritten_text\"]], \"rewrite_prompt\": [\"\"]})\n",
    "        }]\n",
    "        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_output = peft_model.generate(encoded_input, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n",
    "        decoded_output = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", \"\", decoded_output, 1)\n",
    "\n",
    "        res.append([row[\"id\"], decoded_output])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "\n",
    "    finally:\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated prompts\n",
    "sub = pd.DataFrame(res, columns=[\"id\", \"rewrite_prompt\"])\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> a0289740d0ae65cca1bc9594d9014afa0e9fd51d
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
